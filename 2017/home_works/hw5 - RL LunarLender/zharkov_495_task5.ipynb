{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate q-learning\n",
    "\n",
    "In this notebook you will teach a lasagne neural network to do Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Frameworks__ - we'll accept this homework in any deep learning framework. For example, it translates to TensorFlow almost line-to-line. However, we recommend you to stick to theano/lasagne unless you're certain about your skills in the framework of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n"
     ]
    }
   ],
   "source": [
    "%env THEANO_FLAGS='floatX=float32'\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0xf1ad978>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAD8CAYAAAB9y7/cAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x9afa550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "n_actions = env.action_space.n\n",
    "state_dim = env.observation_space.shape\n",
    "\n",
    "plt.imshow(env.render(\"rgb_array\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate (deep) Q-learning: building the network\n",
    "\n",
    "In this section we will build and train naive Q-learning with theano/lasagne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is initializing input variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "#create input variables. We'll support multiple states at once\n",
    "\n",
    "\n",
    "current_states = T.matrix(\"states[batch,units]\")\n",
    "actions = T.ivector(\"action_ids[batch]\")\n",
    "rewards = T.vector(\"rewards[batch]\")\n",
    "next_states = T.matrix(\"next states[batch,units]\")\n",
    "is_end = T.ivector(\"vector[batch] where 1 means that session just ended\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\andrey\\anaconda3\\envs\\py34\\lib\\site-packages\\theano\\tensor\\signal\\downsample.py:6: UserWarning: downsample module has been moved to the theano.tensor.signal.pool module.\n",
      "  \"downsample module has been moved to the theano.tensor.signal.pool module.\")\n"
     ]
    }
   ],
   "source": [
    "from lasagne import nonlinearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "\n",
    "#input layer\n",
    "l_states = InputLayer((None,)+state_dim)\n",
    "\n",
    "nnet = DenseLayer(l_states, 50, nonlinearity=nonlinearities.leaky_rectify)\n",
    "\n",
    "\n",
    "#output layer\n",
    "l_qvalues = DenseLayer(nnet,num_units=n_actions,nonlinearity=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting Q-values for `current_states`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get q-values for ALL actions in current_states\n",
    "predicted_qvalues = get_output(l_qvalues,{l_states:current_states})\n",
    "\n",
    "#predict q-values for next states\n",
    "predicted_next_qvalues = get_output(l_qvalues,{l_states:next_states})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compiling agent's \"GetQValues\" function\n",
    "get_qvalues = theano.function([current_states], predicted_qvalues, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#select q-values for chosen actions\n",
    "predicted_qvalues_for_actions = predicted_qvalues[T.arange(actions.shape[0]),actions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loss function and `update`\n",
    "Here we write a function similar to `agent.update`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing target q-values under \n",
    "gamma = 0.99\n",
    "target_qvalues_for_actions = rewards + gamma * predicted_next_qvalues.max(axis=1)\n",
    "\n",
    "#zero-out q-values at the end\n",
    "target_qvalues_for_actions = (1-is_end)*target_qvalues_for_actions\n",
    "\n",
    "#don't compute gradient over target q-values (consider constant)\n",
    "target_qvalues_for_actions = theano.gradient.disconnected_grad(target_qvalues_for_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mean squared error loss function\n",
    "loss = ((target_qvalues_for_actions - predicted_qvalues_for_actions)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all network weights\n",
    "all_weights = get_all_params(l_qvalues,trainable=True)\n",
    "\n",
    "#network updates. Note the small learning rate (for stability)\n",
    "updates = lasagne.updates.sgd(loss,all_weights,learning_rate=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training function that resembles agent.update(state,action,reward,next_state) \n",
    "#with 1 more argument meaning is_end\n",
    "train_step = theano.function([current_states,actions,rewards,next_states,is_end],\n",
    "                             updates=updates, allow_input_downcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.25 #initial epsilon\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with approximate q-learning agent and train it at the same time\"\"\"\n",
    "    \n",
    "    total_reward = 0\n",
    "    s = env.reset()\n",
    "    \n",
    "    for t in range(t_max):\n",
    "        \n",
    "        #get action q-values from the network\n",
    "        q_values = get_qvalues([s])[0] \n",
    "        \n",
    "        if np.random.random() < epsilon:\n",
    "            a = np.random.randint(0, n_actions) \n",
    "        else:\n",
    "            a = q_values.argmax()\n",
    "            \n",
    "        new_s,r,done,info = env.step(a)\n",
    "        \n",
    "        #train agent one step. Note that we use one-element arrays instead of scalars \n",
    "        #because that's what function accepts.\n",
    "        train_step([s],[a],[r],[new_s],[done])\n",
    "        \n",
    "        total_reward+=r\n",
    "        \n",
    "        s = new_s\n",
    "        if done: break\n",
    "            \n",
    "    return total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тут как повезет\n",
    "\n",
    "В самый первый раз reward дошел до 14 почти сразу\n",
    "\n",
    "Потом я зачем-то перезапустил и выше -100 почти никогда не поднималось\n",
    "\n",
    "Это второй раз \"повезло\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean reward:-272.247\tepsilon:0.00950\n",
      "mean reward:-251.082\tepsilon:0.00903\n",
      "mean reward:-223.689\tepsilon:0.00857\n",
      "mean reward:-260.154\tepsilon:0.00815\n",
      "mean reward:-199.295\tepsilon:0.00774\n",
      "mean reward:-189.398\tepsilon:0.00735\n",
      "mean reward:-209.859\tepsilon:0.00698\n",
      "mean reward:-200.322\tepsilon:0.00663\n",
      "mean reward:-199.872\tepsilon:0.00630\n",
      "mean reward:-221.203\tepsilon:0.00599\n",
      "mean reward:-205.851\tepsilon:0.00569\n",
      "mean reward:-175.687\tepsilon:0.00540\n",
      "mean reward:-178.327\tepsilon:0.00513\n",
      "mean reward:-145.599\tepsilon:0.00488\n",
      "mean reward:-98.695\tepsilon:0.00463\n",
      "mean reward:-95.002\tepsilon:0.00440\n",
      "mean reward:-102.384\tepsilon:0.00418\n",
      "mean reward:-125.910\tepsilon:0.00397\n",
      "mean reward:-109.117\tepsilon:0.00377\n",
      "mean reward:-115.709\tepsilon:0.00358\n",
      "mean reward:-113.332\tepsilon:0.00341\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-dfbe37621048>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#generate new sessions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mepsilon\u001b[0m\u001b[1;33m*=\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-16-dfbe37621048>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mrewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#generate new sessions\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mepsilon\u001b[0m\u001b[1;33m*=\u001b[0m\u001b[1;36m0.95\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-00eb6fe40024>\u001b[0m in \u001b[0;36mgenerate_session\u001b[1;34m(t_max)\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m#train agent one step. Note that we use one-element arrays instead of scalars\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m#because that's what function accepts.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnew_s\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mtotal_reward\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrey\\anaconda3\\envs\\py34\\lib\\site-packages\\theano\\compile\\function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    858\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 859\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    860\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'position_of_error'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrey\\anaconda3\\envs\\py34\\lib\\site-packages\\theano\\gof\\op.py\u001b[0m in \u001b[0;36mrval\u001b[1;34m(p, i, o, n)\u001b[0m\n\u001b[0;32m    910\u001b[0m             \u001b[1;31m# default arguments are stored in the closure of `rval`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    911\u001b[0m             \u001b[1;32mdef\u001b[0m \u001b[0mrval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_input_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode_output_storage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 912\u001b[1;33m                 \u001b[0mr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    913\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    914\u001b[0m                     \u001b[0mcompute_map\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mo\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\andrey\\anaconda3\\envs\\py34\\lib\\site-packages\\theano\\tensor\\subtensor.py\u001b[0m in \u001b[0;36mperform\u001b[1;34m(self, node, inputs, out_)\u001b[0m\n\u001b[0;32m   2164\u001b[0m         \u001b[1;31m# TODO: in general, we need to re-pack the inputs into a valid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2165\u001b[0m         \u001b[1;31m# index, just like subtensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2166\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2167\u001b[0m         if (numpy.__version__ <= '1.6.1' and\n\u001b[0;32m   2168\u001b[0m                 out[0].size != numpy.uint32(out[0].size)):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epsilon = 0.01\n",
    "for i in range(200):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    epsilon*=0.95\n",
    "    \n",
    "    print (\"mean reward:%.3f\\tepsilon:%.5f\"%(np.mean(rewards),epsilon))\n",
    "\n",
    "    if np.mean(rewards) > 0:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon = 0.01\n",
    "for i in range(100):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    epsilon*=0.95\n",
    "    \n",
    "    print (\"mean reward:%.3f\\tepsilon:%.5f\"%(np.mean(rewards),epsilon))\n",
    "\n",
    "    if np.mean(rewards) > 0:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#epsilon = 0.01\n",
    "for i in range(10):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)] #generate new sessions\n",
    "    \n",
    "    epsilon*=0.95\n",
    "    \n",
    "    print (\"mean reward:%.3f\\tepsilon:%.5f\"%(np.mean(rewards),epsilon))\n",
    "\n",
    "    if np.mean(rewards) > 0:\n",
    "        print (\"You Win!\")\n",
    "        break\n",
    "        \n",
    "    assert epsilon!=0, \"Please explore environment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon=0 #Don't forget to reset epsilon back to initial value if you want to go on training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=gym.make(\"LunarLander-v2\");env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#record sessions\n",
    "import gym.wrappers\n",
    "env = gym.wrappers.Monitor(env,directory=\"videos\",force=True)\n",
    "sessions = [generate_session() for _ in range(100)]\n",
    "env.close()\n",
    "#unwrap \n",
    "env = env.env.env\n",
    "#upload to gym\n",
    "#gym.upload(\"./videos/\",api_key=\"<your_api_key>\") #you'll need me later\n",
    "\n",
    "#Warning! If you keep seeing error that reads something like\"DoubleWrapError\",\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./videos/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./videos/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
