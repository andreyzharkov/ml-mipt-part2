{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple q-learning agent with experience replay\n",
    "\n",
    "We re-write q-learning algorithm using _agentnet_ - a helper for lasagne that implements some RL techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! pip install --upgrade https://github.com/yandexdataschool/AgentNet/archive/master.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: THEANO_FLAGS='floatX=float32'\n",
      "env: DISPLAY=:1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"bash\" ­Ґ пў«пҐвбп ў­гваҐ­­Ґ© Ё«Ё ў­Ґи­Ґ©\n",
      "Є®¬ ­¤®©, ЁбЇ®«­пҐ¬®© Їа®Ја ¬¬®© Ё«Ё Ї ЄҐв­л¬ д ©«®¬.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "%env THEANO_FLAGS='floatX=float32'\n",
    "\n",
    "#XVFB will be launched if you run on a server\n",
    "import os\n",
    "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
    "    !bash ../xvfb start\n",
    "    %env DISPLAY=:1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment setup\n",
    "* Here we simply load the game and check that it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "make_env = lambda: gym.make(\"Acrobot-v1\")\n",
    "\n",
    "env=make_env()\n",
    "env.reset()\n",
    "\n",
    "state_shape = env.observation_space.shape\n",
    "n_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQsAAAD8CAYAAABgtYFHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkroYC0ldjIWkLsZCUhdjIamLsZDU5f8BSonYnPEDbp0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23f7342b6d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(env.render(\"rgb_array\"))\n",
    "del env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import *\n",
    "#from lasagne.nonlinearities import elu\n",
    "\n",
    "\n",
    "#image observation at current tick goes here, shape = (sample_i,x,y,color)\n",
    "observation_layer = InputLayer((None,)+state_shape)\n",
    "\n",
    "#a layer that predicts Qvalues\n",
    "qvalues_layer = DenseLayer(observation_layer,num_units=100,\n",
    "                           nonlinearity=None,name=\"q-values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking actions is done by yet another layer, that implements $ \\epsilon$ -greedy policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'standardize'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3af95e749b8f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0magentnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolver\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEpsilonGreedyResolver\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0maction_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEpsilonGreedyResolver\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqvalues_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#set starting epsilon\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0maction_layer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.05\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Andrey\\Anaconda3\\lib\\site-packages\\agentnet\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0menvironment\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mobjective\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mresolver\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Andrey\\Anaconda3\\lib\\site-packages\\agentnet\\memory\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mgru\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGRUMemoryLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlogical\u001b[0m \u001b[1;32mimport\u001b[0m  \u001b[0mCounterLayer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSwitchLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mattention\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAttentionLayer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDotAttentionLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Andrey\\Anaconda3\\lib\\site-packages\\agentnet\\memory\\attention.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDictLayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0minit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlasagne\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDenseLayer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mSliceLayer\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mstandardize\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mAttentionLayer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDictLayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'standardize'"
     ]
    }
   ],
   "source": [
    "from agentnet.resolver import EpsilonGreedyResolver\n",
    "action_layer = EpsilonGreedyResolver(qvalues_layer)\n",
    "\n",
    "#set starting epsilon\n",
    "action_layer.epsilon.set_value(np.float32(0.05))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent\n",
    "\n",
    "We define an agent entirely composed of a lasagne network:\n",
    "* Observations as InputLayer(s)\n",
    "* Actions as intermediate Layer(s)\n",
    "* `policy_estimators` is \"whatever else you want to keep track of\"\n",
    "\n",
    "Each parameter can be either one layer or a list of layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.agent import Agent\n",
    "agent = Agent(observation_layers=<which layer?>,\n",
    "              action_layers=<which layer?>\n",
    "              policy_estimators=qvalues_layer,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Since it's a single lasagne network, one can get it's weights, output, etc\n",
    "weights = lasagne.layers.get_all_params(action_layer,trainable=True)\n",
    "weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create and manage a pool of atari sessions to play with\n",
    "\n",
    "* To make training more stable, we shall have an entire batch of game sessions each happening independent of others\n",
    "* Why several parallel agents help training: http://arxiv.org/pdf/1602.01783v1.pdf\n",
    "* Alternative approach: store more sessions: https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from agentnet.experiments.openai_gym.pool import EnvPool\n",
    "pool = EnvPool(agent,make_env,n_games=1,max_size=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#interact for 7 ticks\n",
    "obs_log,action_log,reward_log,_,_,_  = pool.interact(5)\n",
    "\n",
    "\n",
    "print('actions:',action_log)\n",
    "print('rewards:',reward_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#we'll train on rollouts of 10 steps (required by n-step algorithms and rnns later)\n",
    "SEQ_LENGTH=10\n",
    "\n",
    "#load first sessions (this function calls interact and stores sessions in the pool)\n",
    "\n",
    "for _ in range(100):\n",
    "    pool.update(SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# q-learning\n",
    "\n",
    "We shall now define a function that replays recent game sessions and updates network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get agent's Qvalues obtained via experience replay\n",
    "replay = pool.experience_replay.sample_session_batch(100)\n",
    "qvalues_seq = agent.get_sessions(\n",
    "    replay,\n",
    "    session_length=SEQ_LENGTH,\n",
    "    experience_replay=True,\n",
    ")[-1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loss for Qlearning = (Q(s,a) - (r+gamma*Q(s',a_max)))^2, like you implemented before in lasagne.\n",
    "\n",
    "from agentnet.learning import qlearning\n",
    "elwise_mse_loss = qlearning.get_elementwise_objective(qvalues_seq,\n",
    "                                                      replay.actions[0],\n",
    "                                                      replay.rewards,\n",
    "                                                      replay.is_alive,\n",
    "                                                      gamma_or_gammas=0.99,\n",
    "                                                      n_steps=1,)\n",
    "\n",
    "#compute mean loss over \"alive\" fragments\n",
    "loss = elwise_mse_loss.sum() / replay.is_alive.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#get weight updates\n",
    "updates = lasagne.updates.adam(loss,weights,learning_rate=1e-4)\n",
    "\n",
    "#compile train function\n",
    "import theano\n",
    "train_step = theano.function([],loss,updates=updates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo run\n",
    "\n",
    "Play full session with an untrained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for MountainCar-v0 evaluation session is cropped to 200 ticks\n",
    "untrained_reward = pool.evaluate(save_path=\"./records\",record_video=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show video\n",
    "from IPython.display import HTML\n",
    "import os\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./records/\")))\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"480\" controls>\n",
    "  <source src=\"{}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\".format(\"./records/\"+video_names[-1])) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epoch_counter = 1 #starting epoch\n",
    "rewards = {} #full game rewards\n",
    "target_score = -90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "\n",
    "for i in trange(10000):    \n",
    "    \n",
    "    #play\n",
    "    for _ in range(5):\n",
    "        pool.update(SEQ_LENGTH,append=True)\n",
    "    \n",
    "    #train\n",
    "    train_step()\n",
    "    \n",
    "    #update epsilon\n",
    "    epsilon = 0.05 + 0.95*np.exp(-epoch_counter/1000.)\n",
    "    action_layer.epsilon.set_value(np.float32(epsilon))\n",
    "    \n",
    "    #play a few games for evaluation\n",
    "    if epoch_counter%100==0:\n",
    "        rewards[epoch_counter] = np.mean(pool.evaluate(n_games=3,record_video=False))\n",
    "        print(\"iter=%i\\tepsilon=%.3f\"%(epoch_counter,action_layer.epsilon.get_value(),))\n",
    "        print(\"Current score(mean over %i) = %.3f\"%(3,np.mean(rewards[epoch_counter])))\n",
    "    \n",
    "        if rewards[epoch_counter] >= target_score:\n",
    "            print(\"You win!\")\n",
    "            break\n",
    "\n",
    "    \n",
    "    epoch_counter  +=1\n",
    "\n",
    "    \n",
    "# Time to drink some coffee!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas import ewma\n",
    "iters,session_rewards=zip(*sorted(rewards.items(),key=lambda (k,v):k))\n",
    "plt.plot(iters,ewma(np.array(session_rewards),span=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_reward = pool.evaluate(n_games=10,save_path=\"./records\",record_video=True)\n",
    "\n",
    "print(\"average reward:\",final_reward)\n",
    "\n",
    "video_names = list(filter(lambda s:s.endswith(\".mp4\"),os.listdir(\"./records/\")))\n",
    "\n",
    "for video_name in video_names:\n",
    "    HTML(\"\"\"\n",
    "    <video width=\"640\" height=\"480\" controls>\n",
    "      <source src=\"{}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\".format(\"./records/\"+video_name)) #this may or may not be _last_ video. Try other indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
